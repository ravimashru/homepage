{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1611e4d4-93fb-44aa-ba57-4441e4ee87c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convolutions in Fastai\n",
    "\n",
    "[Chapter 13 of Fastbook](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) dives into the details of convolutional neural networks and how the convolution operation that they're built on works.\n",
    "\n",
    "[A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) has excellent low-level details of how different types of convolutions work.\n",
    "\n",
    "In this post, I explore how these different types of convolution operations can be applied with fastai.\n",
    "\n",
    "\n",
    "## Convolution operations\n",
    "\n",
    "The behavior of a convolutional layer depends on the following properties:\n",
    "\n",
    "1. kernel size\n",
    "2. stride\n",
    "3. padding\n",
    "\n",
    "In addition to these, we'll also look at two more properties: `transpose` and `dilation`.\n",
    "\n",
    "Let us create a `5x5` tensor with random values to use as an input for the convolutional layers we will create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c4284d0-eeb5-459b-9c27-c6d965600ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "t = torch.rand(5, 5).unsqueeze(0).unsqueeze(0)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d0065-b496-42da-87b7-cdb7f4222723",
   "metadata": {},
   "source": [
    "Convolution layers expect the first dimension to be the batch size, and the second to be the number of channels. We use the `unsqueeze` function to add a dimension of `1` for these.\n",
    "\n",
    "### No padding and strides of size 1\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b584565f-210a-463e-88fc-6a6e62cbb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = ConvLayer(1, 3, ks=3, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d8741-b68f-4ec9-8363-21947eb66c32",
   "metadata": {},
   "source": [
    "- The first parameter specifies the **number of channels in the input**.\n",
    "- The second parameter specifies **how many filters** we want to create in this layer. This will be equal to the number of channels in the output since each channel is created by one filter.\n",
    "- The `ks` parameter specifies the **size of the filters** we want - `4x4`. We need to provide just one integer because filters are always square. The default value of this parameter is 3.\n",
    "- The `stride` parameter specifies the **size of the stride**. The default value of this parameter is 1.\n",
    "- The `padding` parameter specifies **how much padding** we apply around the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "acc0a859-1005-47d9-93b7-a11ba8ca5e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conv1(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c269e3-4253-43e4-9aad-2952ef0eb6d0",
   "metadata": {},
   "source": [
    "The batch size in the output remains the same as that in the input.\n",
    "\n",
    "The number of channels has increased to 3 since we created a convolutional layer with 3 filters.\n",
    "\n",
    "With no padding and strides of size 1, the dimensions of the output are `input size - kernel size + 1`, which in our case is `5 - 4 + 1 = 2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bfa97-a7e8-42b0-b8f6-2c57f2b3396c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Zero padding and strides of size 1\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42fd8039-e0a4-4750-907f-c7950e7d343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = ConvLayer(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220b83d-07af-440e-b3ac-c2c748ee4ce1",
   "metadata": {},
   "source": [
    "We don't specify the stride parameter since its default value is 1.\n",
    "\n",
    "We haven't specified the `ks` parameter so the default value of 3 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60859bb2-fdb9-4cf0-b41d-c95ea474a9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conv2(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838850ec-642e-49a0-8ed3-65a3ed3fc0a8",
   "metadata": {},
   "source": [
    "**fastai automatically applies an appropriate amount of padding by default** if we are not using transposed convolutions (more about that in a bit) to ensure that our input and output dimensions are equal.\n",
    "\n",
    "This kind of padding is also commonly known as **half padding** and **same padding**.\n",
    "\n",
    "There is another type of padding called **full padding** which allows us to increase the dimensions of the output. Full padding can be achieved by using regular zero padding of size `k-1` (where `k` is the kernel size).\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/full_padding_no_strides.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6834db01-1999-4ecb-8bc8-6f066673aeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 7, 7])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3 = ConvLayer(1, 3, padding=(2,2))\n",
    "\n",
    "res = conv3(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8077fa7-25c5-41a3-bac1-33dccda3cb96",
   "metadata": {},
   "source": [
    "With full padding, the dimensions of the output are `input size + kernel size - 1`, which in this case is `5 + 3 - 1 = 7`.\n",
    "\n",
    "### Strided convolutions\n",
    "\n",
    "By specifying a value for the `stride` parameter greater than 1, we can perform strided convolutions.\n",
    "\n",
    "Strided convolutions are useful for descreasing the dimensions of the output.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11190df6-411b-4295-b340-bacb49ef8208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv4 = ConvLayer(1, 3, stride=2)\n",
    "\n",
    "res = conv4(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbd8e7-11ff-4d93-90b7-9b2bdf3b559f",
   "metadata": {},
   "source": [
    "We can also have strided convolutions with no padding applied to the input.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "302fc7e3-c1fd-4815-a57d-a424bcb5be1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv5 = ConvLayer(1, 3, stride=2, padding=0)\n",
    "\n",
    "res = conv5(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c231923-ac99-4e6f-ac11-0618622dabc3",
   "metadata": {},
   "source": [
    "## Transposed Convolutions\n",
    "\n",
    "Also known as: fractionally strided convolutions, deconvolutions.\n",
    "\n",
    "Transposed convolutions allow us to increase the dimension of the output compared to the input.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides_transposed.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a9637cd-7796-46bb-a584-93025a4cf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv6 = ConvLayer(1, 3, transpose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4a76a-4bda-4807-bbe0-ef87c8762c0e",
   "metadata": {},
   "source": [
    "We can use transposed convolutions in fastai by setting `transpose=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5eeab36-311d-4979-a176-1dcbff30c33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 7, 7])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conv6(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba2001-451b-4847-a68a-8d2ad95061cc",
   "metadata": {},
   "source": [
    "We can also use a stride bigger than 1. To visualize this operation, imagine adding zero padding between all values in the input.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_transposed.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ac84cf9-d14d-4408-9123-ba3fd5447284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 11, 11])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv7 = ConvLayer(1, 3, transpose=True, stride=2)\n",
    "\n",
    "res = conv7(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16b1ea-88bf-4aee-840c-663f19164467",
   "metadata": {},
   "source": [
    "## Dilated Convolutions\n",
    "\n",
    "Regular convolution operations work on elements in the input that are next to each other. Dilated convolutions skip elements in the input.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif)\n",
    "\n",
    "*Image from https://github.com/vdumoulin/conv_arithmetic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a8e169c-1d72-45b9-8ed8-3c74dd13bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv8 = ConvLayer(1, 3, dilation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4b036-86f6-4b36-97fa-eb1dff753bed",
   "metadata": {},
   "source": [
    "The number of elements to skip in dilated convolutions is controlled by the `dilation` parameter.\n",
    "\n",
    "A value of `dilation=1` corresponds to normal convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e531e883-f18e-4630-8c56-a935f78fcb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conv8(t)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8571f9-d2d9-4591-9cf9-dae5f9e004b8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Fastbook - Chapter 13](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb)\n",
    "- [A guide to convolutional arithmetic for deep learning](https://arxiv.org/abs/1603.07285)\n",
    "- [Fastai ConvLayer documentation](https://docs.fast.ai/layers.html#ConvLayer)"
   ]
  }
 ],
 "metadata": {
  "title": "Convolutions in Fastai",
  "date": "2021-08-31",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
